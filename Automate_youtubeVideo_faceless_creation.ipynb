{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d51e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.config as mconf\n",
    "\n",
    "# Replace this path if your magick.exe is elsewhere\n",
    "mconf.change_settings({\n",
    "    \"IMAGEMAGICK_BINARY\": r\"C:\\Program Files\\ImageMagick-7.1.1-Q16-HDRI\\magick.exe\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd0e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import os\n",
    "import textwrap\n",
    "from moviepy.editor import *\n",
    "import subprocess\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# API Keys\n",
    "OPENAI_API_KEY = \"\"\n",
    "# Initialize OpenAI Client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8e1adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_script(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b1271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"You are a professional scriptwriter. Write a clear, engaging, and informative narration script for a faceless YouTube video on the topic: Artificial Intelligence.\n",
    "\n",
    "Output only the voiceover narration in plain text. Do not include any visual scene directions, sound effects, music cues, or character actions.\n",
    "\n",
    "Use a friendly, conversational, and informative tone.\n",
    "\n",
    "The script should be approximately 30 seconds long.\n",
    "\n",
    "Start with: ‚ÄúHi everyone, welcome to Vivek's channel,‚Äù  \n",
    "End with: ‚ÄúSubscribe to my channel and like my videos. Thank you.‚Äù\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Generate script from LLM\n",
    "script = generate_script(PROMPT)\n",
    "print(\"üìù Generated Script:\\n\", script)\n",
    "\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "response = client.audio.speech.create(\n",
    "    model=\"tts-1\",\n",
    "    voice=\"nova\",\n",
    "    input=script,\n",
    ")\n",
    "\n",
    "# Save the audio\n",
    "with open(\"openai_voice.mp3\", \"wb\") as f:\n",
    "    f.write(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07a40b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_slideshow_video(images_folder, audio_path, output_path=\"final_video_mix11.mp4\", fps=1):\n",
    "    # 1. Collect images\n",
    "    image_files = sorted(glob.glob(os.path.join(images_folder, \"*.jpg\")))\n",
    "    if not image_files:\n",
    "        raise FileNotFoundError(\"No PNG images found in folder.\")\n",
    "\n",
    "    # 2. Get audio duration\n",
    "    cmd_probe = [\n",
    "        \"ffprobe\", \"-v\", \"error\", \"-select_streams\", \"a:0\",\n",
    "        \"-show_entries\", \"stream=duration\", \"-of\", \"json\", audio_path\n",
    "    ]\n",
    "    result = subprocess.run(cmd_probe, capture_output=True, text=True)\n",
    "    audio_duration = float(json.loads(result.stdout)[\"streams\"][0][\"duration\"])\n",
    "    print(f\"‚úî Audio duration: {audio_duration:.2f}s\")\n",
    "\n",
    "    # 3. Calculate duration per image\n",
    "    per_image_duration = audio_duration / len(image_files)\n",
    "\n",
    "    # 4. Generate input.txt\n",
    "    with open(\"input.txt\", \"w\") as f:\n",
    "        for img in image_files:\n",
    "            f.write(f\"file '{img}'\\n\")\n",
    "            f.write(f\"duration {per_image_duration:.2f}\\n\")\n",
    "        f.write(f\"file '{image_files[-1]}'\\n\")  # final image stays at end\n",
    "\n",
    "    # 5. Run FFmpeg (ensure -vsync vfr to respect duration)\n",
    "    cmd = [\n",
    "        \"ffmpeg\", \"-y\",\n",
    "        \"-f\", \"concat\", \"-safe\", \"0\", \"-i\", \"input.txt\",\n",
    "        \"-i\", audio_path,\n",
    "        \"-vsync\", \"vfr\",  # respect variable frame rate\n",
    "        \"-pix_fmt\", \"yuv420p\",\n",
    "        \"-vf\", \"scale=iw:-2\",\n",
    "        \"-c:v\", \"libx264\", \"-r\", \"24\",\n",
    "        \"-c:a\", \"aac\", \"-shortest\",\n",
    "        output_path\n",
    "    ]\n",
    "\n",
    "    print(\"‚Üí Running FFmpeg with variable frame rate...\")\n",
    "    proc = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    print(proc.stderr)\n",
    "    proc.check_returncode()\n",
    "    print(f\"‚úÖ Video created successfully: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f002b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#31-07, 1.25\n",
    "create_slideshow_video(\"images\", \"voice.mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5930f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_background_music(url, filename=\"background_music1.mp3\"):\n",
    "    response = requests.get(url)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"‚úî Downloaded background music to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9040516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "def mix_audio(voice_path, music_path, output_path=\"mixed_audio1.mp3\", music_volume_dB=-15):\n",
    "    voice = AudioSegment.from_file(voice_path)\n",
    "    music = AudioSegment.from_file(music_path).apply_gain(music_volume_dB)\n",
    "    music = music[:len(voice)]  # Trim music to voice length\n",
    "\n",
    "    mixed = voice.overlay(music)\n",
    "    mixed.export(output_path, format=\"mp3\")\n",
    "    print(f\"‚úî Mixed audio saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download music\n",
    "download_background_music(\"https://www.soundhelix.com/examples/mp3/SoundHelix-Song-1.mp3\")\n",
    "\n",
    "# Step 2: Mix music with voice\n",
    "mix_audio(\"openai_voice.mp3\", \"background_music1.mp3\", output_path=\"mixed_audio1.mp3\")\n",
    "\n",
    "# Step 3: Create video\n",
    "#create_video_with_audio_folder(\"images\", \"mixed_audio.mp3\")\n",
    "create_slideshow_video(\"images\", \"mixed_audio1.mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c371797",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Creating the subtitles for the video\n",
    "\n",
    "def generate_and_mux_subtitles(\n",
    "    video_in: str,\n",
    "    audio_path: str,\n",
    "    adjusted_srt: str = \"adjusted_subtitles.srt\",\n",
    "    final_out: str = \"video_with_subs.mp4\"\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Transcribe TTS audio ‚Üí raw SRT via Whisper\n",
    "    2) Auto-scale subtitle timings to match audio/video lengths\n",
    "    3) Mux as soft subtitles (mov_text) into final MP4\n",
    "    \"\"\"\n",
    "    # ‚Äî‚Äî‚Äî 1) Whisper transcription ‚Üí raw_srt ‚Äî‚Äî‚Äî\n",
    "    with open(\"openai_voice.mp3\", \"rb\") as audio_file:\n",
    "     transcript = client.audio.transcriptions.create(\n",
    "        file=audio_file,\n",
    "        model=\"whisper-1\",\n",
    "        response_format=\"srt\"\n",
    "    )\n",
    "\n",
    "# Save the subtitle file\n",
    "    raw_srt=\"subtitles.srt\"\n",
    "    with open(\"subtitles.srt\", \"w\", encoding=\"utf-8\") as f:\n",
    "     f.write(transcript)\n",
    "    # ‚Äî‚Äî‚Äî 2) Adjust timings ‚Äî‚Äî‚Äî\n",
    "    # get durations\n",
    "    vid_info   = mediainfo(video_in)\n",
    "    aud_info   = mediainfo(audio_path)\n",
    "    video_dur  = float(vid_info[\"duration\"])\n",
    "    audio_dur  = float(aud_info[\"duration\"])\n",
    "    scale      = audio_dur / video_dur\n",
    "\n",
    "    subs = pysubs2.load(raw_srt)\n",
    "    for ev in subs:\n",
    "        ev.start = int(ev.start * scale)\n",
    "        ev.end   = int(ev.end   * scale)\n",
    "    subs.save(adjusted_srt)\n",
    "    print(f\"‚úÖ Adjusted subtitles saved ‚Üí {adjusted_srt}\")\n",
    "\n",
    "    # ‚Äî‚Äî‚Äî 3) Mux as soft subs ‚Äî‚Äî‚Äî\n",
    "    cmd = [\n",
    "        \"ffmpeg\", \"-y\",\n",
    "        \"-i\", video_in,\n",
    "        \"-i\", adjusted_srt,\n",
    "        \"-c:v\", \"copy\",\n",
    "        \"-c:a\", \"copy\",\n",
    "        \"-c:s\", \"mov_text\",\n",
    "        final_out\n",
    "    ]\n",
    "    print(\"‚Üí Running:\\n  \" + \" \".join(cmd))\n",
    "    subprocess.run(cmd, check=True)\n",
    "    print(f\"‚úÖ Final video with subtitles ‚Üí {final_out}\")\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a411502",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_mux_subtitles(\n",
    "        video_in=\"final_video_mix11.mp4\",\n",
    "        audio_path=\"openai_voice.mp3\",\n",
    "        adjusted_srt=\"adjusted_subtitles.srt\",\n",
    "        final_out=\"video_with_adjusted_subs1.mp4\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt_auto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
